{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strings & Encoding\n",
    "\n",
    "---\n",
    "\n",
    "The standard encoding which used 16 bit code for characters was not enough for the Unicode dictionary. And using twice as much memory was not an easy choice. Thus came UTF-16. What it does is use 16 bit data for the most common characters but for the rare ones, two such sets. This is a bad idea.\n",
    "> There is a lack of consistency. JS was already written to work on using a 16-bit character code to handle strings. Some features broke, down for the characters involving double code such are some Chinese and emojis. See example.\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "ï¿½\n",
      "55357\n",
      "128052\n"
     ]
    }
   ],
   "source": [
    "var horseShoe=\"ðŸ´ðŸ‘Ÿ\";\n",
    "console.log(horseShoe.length);\n",
    "console.log(horseShoe[0]);\n",
    "console.log(horseShoe.charCodeAt(0));\n",
    "console.log(horseShoe.codePointAt(0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "What happened here needs to be looked at. The first is pretty obvious. The next fails to return a character because the code at 0th index is only a half character. The next line gives that half character/ half code. The last line though gives the code for the full character.\n",
    "\n",
    ">`for-of` loop is safe to use here. It is unicode aware and will return a full character.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JavaScript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "20.16.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
